{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNLP: Lab Session 6\n",
    "# Pointwise Mutual Information - Finding Collocations\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aims of this lab session are to\n",
    "  1. familiarize you with pointwise mutual information (PMI);\n",
    "  2. show how to apply PMI for the task of finding word collocations;\n",
    "  3. identify shortcomings of this approach.\n",
    "\n",
    "By the end of this lab session, you should be able to:\n",
    "* Compute the PMI.\n",
    "* Apply PMI to find word collocations in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lab we consider the task of identifying word collocations in a corpus to demonstrate the use of Pointwise\n",
    "Mutual Information (PMI).\n",
    "\n",
    "$PMI(x_i, y_j) = log\\frac{P(X=x_i,Y=y_j)}{P(X=x_i)P(Y =y_j)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.probability import FreqDist, MLEProbDist\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import stopwords\n",
    "from math import log\n",
    "from pprint import pprint\n",
    "\n",
    "class FasterMLEProbDist(MLEProbDist):\n",
    "    '''Speed up prob lookup for large sample sizes'''\n",
    "    def __init__(self,freqdist):\n",
    "        '''\n",
    "        :param freqdist: samples and their counts\n",
    "        :type freqdist: nltk.probability.FreqDist\n",
    "        :return: A (slightly) faster probability distribution\n",
    "        :rtype: FasterMLEProbDist\n",
    "        '''\n",
    "        self._N = freqdist.N()\n",
    "        if self._N == 0:\n",
    "            self._empty = True\n",
    "        else:\n",
    "            self._empty = False\n",
    "            self._pq=float(self._N)\n",
    "            MLEProbDist.__init__(self, freqdist)\n",
    "\n",
    "    def prob(self, sample):\n",
    "        '''Faster version of MLEProbDist.prob, using cached quotient for division\n",
    "        \n",
    "        :param sample: The sample to look up\n",
    "        :type sample: [anything]\n",
    "        :return: MLE estimate of probability of sample given distribution\n",
    "        :rtype: float\n",
    "        '''\n",
    "        if self._empty:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(self._freqdist[sample]) / self._pq\n",
    "\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "def Filter1(word):\n",
    "    '''Test for all-alpha string\n",
    "    \n",
    "    :param word: word to check\n",
    "    :type word: str\n",
    "    :return: True iff no non-alpha chars in word\n",
    "    :rtype: bool\n",
    "    '''\n",
    "    return word.isalpha()\n",
    "\n",
    "def Filter2(word):\n",
    "    '''Test for all-alpha string not in stopwords list\n",
    "    \n",
    "    :param word: word to check, should be all lowercase\n",
    "    :type word: str\n",
    "    :return: True iff no non-alpha chars in word and not an nltk English stopword\n",
    "    :rtype: bool\n",
    "    '''\n",
    "    return (word.isalpha() and word not in STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of bigrams extracted from the Herman Melville's novel _Moby Dick_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']'], ['ETYMOLOGY', '.'], ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = gutenberg.sents('melville-moby_dick.txt')\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the Joint and Marginal Probability Distributions\n",
    "\n",
    "In order to compute the PMI we need the joint probability $P(X = x, Y = y)$ and the marginal probabilities $P (X = x)$ and $P (Y = y)$. In our case $P (X = x)$ and $P (Y = y)$ will be the unigram probabilities of the two\n",
    "words that are considered to form a collocation, and $P (X = x, Y = y)$ will be the bigram probability.\n",
    "\n",
    "### Exercise 1:\n",
    "\n",
    "In this exercise we will compute the joint and marginal probability distributions for the word bigrams. You will\n",
    "have to fill in two functions to achieve this:\n",
    "* The function `BuildData` receives as parameters a list of sentences and the name of a Filter function. Two Filter functions are already defined, one eliminates just non-alphanumeric tokens, the other eliminates stop-words as well.\n",
    "* The helper function ex1 should return a list of bigrams and a list of unigrams extracted from the sentences.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "1. Build the two data structures in the BuildData function. Lowercase the words and eliminate unigrams and bigrams that do not pass the filter.\n",
    "  Remember, help is your friend:\n",
    "    `help(nltk.bigrams)`\n",
    "2. The function ex1 receives as parameters a list of bigrams and a list of unigrams and returns the corresponding probability distributions. Construct a `FreqDist` for each of the two lists. Transform each `FreqDist` into a probability distribution using the `FasterMLEProbDist` estimator.\n",
    "  Again, help is your friend:\n",
    "    `help(FasterMLEProbDist)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildData(sentences, filter):\n",
    "    '''Tabulate token unigrams and bigrams lowercased from sentences, filtered\n",
    "    \n",
    "    :param sentences: corpus of sentences\n",
    "    :type sentences: list(list(str))\n",
    "    :param filter: filter to test tokens for inclusion\n",
    "    :type filter: function(str)==>bool\n",
    "    \n",
    "    :return: unigrams and bigrams, lowercased, filtered, from sentences\n",
    "    :rtype: tuple(iterable(tuple(str,str)),iterable(str))'''\n",
    "    # TODO: build the lists of unigrams and bigrams from the sentences\n",
    "    unigrams = [fw for fw in ((lambda lw:lw if filter(lw) else None)(w.lower()) \n",
    "                              for s in sentences for w in s)\n",
    "                if fw is not None]\n",
    "    bigrams = [bigram for bigram in nltk.bigrams(unigrams)]\n",
    "    return bigrams, unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: using the data build the probability distribution over bigrams and unigrams using FasterMLEProbDist\n",
    "def my_prob_distributions(bigrams, unigrams):\n",
    "    '''Build probability distributions from bigram and unigram sequences\n",
    "    \n",
    "    :param bigrams: sequence of pairs of tokens\n",
    "    :type bigrams: iterable(tuple(str,str))\n",
    "    :param unigrams: sequence of tokens\n",
    "    :type unigrams: iterable(str)\n",
    "    \n",
    "    :return: MLE probability distributions for the two inputs\n",
    "    :rtype tuple(FasterMLEProbDist,FasterMLEProbDist)'''\n",
    "    # TODO build the frequency distribution over bigrams and unigrams\n",
    "    bigramFreqDist = FreqDist(bigrams)\n",
    "    unigramFreqDist = FreqDist(unigrams)\n",
    "\n",
    "    # TODO build the probability distribuition from the above frequency distributions using the FasterMLEProbDist estimator\n",
    "    bigramProbDist = FasterMLEProbDist(bigramFreqDist)\n",
    "    unigramProbDist = FasterMLEProbDist(unigramFreqDist)\n",
    "\n",
    "    return bigramProbDist, unigramProbDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the test code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Probability Distribution (type): <class '__main__.FasterMLEProbDist'>\n",
      "Bigram Probability Distribution (type): <class '__main__.FasterMLEProbDist'>\n",
      "Bigrams sorted by MLE (type): <class 'list'>\n",
      "\n",
      "Using filter 1:\n",
      "[(('of', 'the'), 1879),\n",
      " (('in', 'the'), 1182),\n",
      " (('to', 'the'), 731),\n",
      " (('from', 'the'), 440),\n",
      " (('the', 'whale'), 407),\n",
      " (('of', 'his'), 373),\n",
      " (('and', 'the'), 370),\n",
      " (('on', 'the'), 359),\n",
      " (('of', 'a'), 334),\n",
      " (('at', 'the'), 330),\n",
      " (('to', 'be'), 329),\n",
      " (('by', 'the'), 320),\n",
      " (('with', 'the'), 312),\n",
      " (('for', 'the'), 307),\n",
      " (('it', 'was'), 295),\n",
      " (('it', 'is'), 285),\n",
      " (('in', 'his'), 262),\n",
      " (('in', 'a'), 261),\n",
      " (('the', 'ship'), 250),\n",
      " (('with', 'a'), 248),\n",
      " (('into', 'the'), 246),\n",
      " (('the', 'sea'), 228),\n",
      " (('that', 'the'), 220),\n",
      " (('upon', 'the'), 220),\n",
      " (('as', 'the'), 217),\n",
      " (('all', 'the'), 210),\n",
      " (('sperm', 'whale'), 182),\n",
      " (('out', 'of'), 175),\n",
      " (('for', 'a'), 169),\n",
      " (('the', 'same'), 163)]\n",
      "Using filter 2:\n",
      "[(('sperm', 'whale'), 182),\n",
      " (('white', 'whale'), 106),\n",
      " (('moby', 'dick'), 84),\n",
      " (('old', 'man'), 81),\n",
      " (('captain', 'ahab'), 64),\n",
      " (('right', 'whale'), 57),\n",
      " (('mast', 'head'), 49),\n",
      " (('whale', 'ship'), 37),\n",
      " (('mast', 'heads'), 37),\n",
      " (('ye', 'see'), 37),\n",
      " (('cried', 'ahab'), 33),\n",
      " (('captain', 'peleg'), 32),\n",
      " (('aye', 'aye'), 32),\n",
      " (('quarter', 'deck'), 31),\n",
      " (('mr', 'starbuck'), 29),\n",
      " (('one', 'hand'), 28),\n",
      " (('look', 'ye'), 28),\n",
      " (('let', 'us'), 27),\n",
      " (('whale', 'boat'), 27),\n",
      " (('every', 'one'), 24),\n",
      " (('one', 'side'), 23),\n",
      " (('whale', 'head'), 23),\n",
      " (('cried', 'stubb'), 23),\n",
      " (('never', 'mind'), 22),\n",
      " (('whale', 'fishery'), 21),\n",
      " (('thou', 'art'), 21),\n",
      " (('ye', 'ye'), 20),\n",
      " (('chief', 'mate'), 19),\n",
      " (('years', 'ago'), 18),\n",
      " (('sperm', 'whales'), 18)]\n"
     ]
    }
   ],
   "source": [
    "def test_prob_distributions():\n",
    "    '''Test ex1'''\n",
    "    bigrams, unigrams = BuildData(sentences, Filter1)\n",
    "\n",
    "    bigramProbDist1, unigramProbist1 = my_prob_distributions(bigrams, unigrams)\n",
    "    print(\"Unigram Probability Distribution (type): %s\"%type(unigramProbist1))\n",
    "    print(\"Bigram Probability Distribution (type): %s\"%type(bigramProbDist1))\n",
    "    \n",
    "    MLESorted = bigramProbDist1.freqdist().most_common(30)\n",
    "    print(\"Bigrams sorted by MLE (type): %s\"%type(MLESorted))\n",
    "    print()\n",
    "    print(\"Using filter 1:\")\n",
    "    pprint(MLESorted)\n",
    "\n",
    "    bigrams, unigrams = BuildData(sentences, Filter2)\n",
    "    bigramProbDist, unigramProbist = my_prob_distributions(bigrams, unigrams)\n",
    "    MLESorted = bigramProbDist.freqdist().most_common(30)\n",
    "    print(\"Using filter 2:\")\n",
    "    pprint(MLESorted)\n",
    "\n",
    "    return bigramProbDist1, unigramProbist1\n",
    "\n",
    "# TEST EXERCISE 1 - return values will be used for exercise 2\n",
    "bigramProbDist, unigramProbDist = test_prob_distributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the top 30 most frequent bigrams arising from the two different filters.\n",
    "\n",
    "Most of the former are made up of closed-class words. If we eliminate stopwords some interesting bigrams over content words show up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Pointwise Mutual Information\n",
    "\n",
    "In the previous section we estimated the joint and marginal probability distributions from the data. In this\n",
    "section we use these distributions to compute the PMI for a given sample. In order to avoid multiplication of\n",
    "small floating point numbers (probabilities), we can rewrite the formula for PMI as:\n",
    "$P M I(x_i , y_j ) = log P (x_i , y_j ) − (log P (x_i ) + log P (y_j ))$\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "The template of the function that you have to implement takes two parameters: the bigram and unigram\n",
    "probability distributions.\n",
    "1. Create a list of pairs of samples in the distribution and their PMI, using `FasterMLEProbDist.logprob` to get values for unigrams and bigrams from the two FasterMLEProbDist instances.\n",
    "2. Make a dictionary from that list\n",
    "3. Sort the list of pairs in descending order or PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputePMI(bpd, upd):\n",
    "    '''Compute PMI for bigrams from a corpus, sorted in descending order\n",
    "    \n",
    "    :param bpd: Bigram probability distribution\n",
    "    :type bpd: nltk.probability.MLEProbDist\n",
    "    :param upd: Unigram probability distribution\n",
    "    :type upd: nltk.probability.MLEProbDist\n",
    "    :return: samples and their PMIs\n",
    "    :rtype: tuple(dict,list(tuple(str,float)))\n",
    "    '''\n",
    "    \n",
    "    PMIs = [(sample,\n",
    "            bpd.logprob(sample) - (upd.logprob(sample[0]) + upd.logprob(sample[1])))\n",
    "           for sample in bpd.samples()] # TODO: compute list of (sample,PMI) pair for every bigram in bpd\n",
    "\n",
    "    PMIsorted = sorted(PMIs,key=operator.itemgetter(1), reverse=True)\n",
    "    return dict(PMIsorted), PMIsorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the test code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpd type: <class '__main__.FasterMLEProbDist'>\n",
      "upd type: <class '__main__.FasterMLEProbDist'>\n",
      "PMIs type: <class 'dict'>\n",
      "PMIsorted type: <class 'list'>\n",
      "Some illustrative pairs:\n",
      "sperm whale 7.05\n",
      "of the 2.11\n",
      "old man 6.22\n",
      "one side 4.65\n",
      "\n",
      "Top 10 by PMI\n",
      "PMI\tn\tpair\n",
      "17.74\t1\t('herman', 'melville')\n",
      "17.74\t1\t('melville', 'etymology')\n",
      "17.74\t1\t('ger', 'wallen')\n",
      "17.74\t1\t('walw', 'ian')\n",
      "17.74\t1\t('whoel', 'anglo')\n",
      "17.74\t1\t('hwal', 'swedish')\n",
      "17.74\t1\t('painstaking', 'burrower')\n",
      "17.74\t1\t('higgledy', 'piggledy')\n",
      "17.74\t1\t('gudgeon', 'retires')\n",
      "17.74\t1\t('montaigne', 'apology')\n"
     ]
    }
   ],
   "source": [
    "def test_compute_pmi(bpd, upd):\n",
    "    '''Test ex2 with values from test1'''\n",
    "  \n",
    "    print(\"bpd type: %s\"%type(bpd))\n",
    "    print(\"upd type: %s\"%type(upd))\n",
    "\n",
    "    PMIs, PMIsorted = ComputePMI(bpd, upd)\n",
    "    print(\"PMIs type: %s\"%type(PMIs))\n",
    "    print(\"PMIsorted type: %s\"%type(PMIsorted))\n",
    "\n",
    "    print(\"Some illustrative pairs:\\nsperm whale %0.2f\" % PMIs[(\"sperm\",\"whale\")])\n",
    "    print(\"of the %0.2f\" % PMIs[(\"of\",\"the\")])\n",
    "    print(\"old man %0.2f\" % PMIs[(\"old\",\"man\")])\n",
    "    print(\"one side %0.2f\" % PMIs[(\"one\",\"side\")])\n",
    "    \n",
    "    bcount = bpd.freqdist()\n",
    "    print(\"\\nTop 10 by PMI\")\n",
    "    print(\"%s\\t%s\\t%s\"%('PMI','n','pair'))\n",
    "    for pair in PMIsorted[:10]:\n",
    "        print(\"%0.2f\\t%d\\t%s\" % (pair[1], bcount[pair[0]], pair[0]))\n",
    "\n",
    "    return PMIsorted\n",
    "\n",
    "# TEST EXERCISE 2 - return values will be used for exercise 3\n",
    "PMIsorted = test_compute_pmi(bigramProbDist, unigramProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the PMI and bigrams above. We can see that the PMI for _sperm whale_ is 5 binary orders of magnitude greater than the PMI of ”of the”. Can you think of some reasons why the PMI for _old man_ is not as low as we would expect?\n",
    "\n",
    "What can you observe by looking at the top 10 bigrams according to the PMI? How do low counts affect the PMI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "In the previous exercise we found that the PMI is very sensitive to data sparsity. Bigrams composed of low\n",
    "frequency words are ranked higher than bigrams with high frequency words according to PMI. One way to fix\n",
    "this issue is by putting a threshold for the frequency of words.\n",
    "Edit the `filtered_pmi` function to do this:\n",
    "1. Filter the full list of bigrams and their corresponding PMI to include only bigrams with frequency greater than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_pmi(samplesAndPMIs, bpd):\n",
    "    '''Filter a list of sample and PMIs to include only those samples with frequency over 30\n",
    "    \n",
    "    :param samplesAndPMIs: samples and their PMIs\n",
    "    :type samplesAndPMIs: list(tuple(str,float))\n",
    "    :param bpd: probability distribution for the samples\n",
    "    :type bpd: nltk.probability.MLEProbDist\n",
    "    :return: filtered list of samples, PMIs and the sample frequency\n",
    "    :rtype: list(tuple(str,float,int))'''\n",
    "    \n",
    "    bcount = bpd.freqdist() # help(MLEProbDist) is your friend\n",
    "    return [(t,pmi,bcount[t]) for t,pmi in samplesAndPMIs if bcount[t]>30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 by PMI where pair count>30\n",
      "PMI\tn\tpair\n",
      "11.34\t84\t('moby', 'dick')\n",
      "9.55\t32\t('won', 't')\n",
      "9.55\t33\t('didn', 't')\n",
      "9.52\t31\t('quarter', 'deck')\n",
      "9.51\t37\t('mast', 'heads')\n",
      "9.33\t119\t('don', 't')\n",
      "8.18\t32\t('aye', 'aye')\n",
      "8.16\t32\t('captain', 'peleg')\n",
      "7.91\t49\t('mast', 'head')\n",
      "7.71\t51\t('each', 'other')\n",
      "7.69\t33\t('d', 'ye')\n",
      "7.20\t53\t('on', 'board')\n",
      "7.05\t182\t('sperm', 'whale')\n",
      "6.90\t55\t('at', 'least')\n",
      "6.81\t39\t('once', 'more')\n",
      "6.49\t50\t('has', 'been')\n",
      "6.46\t32\t('cried', 'ahab')\n",
      "6.45\t40\t('let', 'me')\n",
      "6.43\t144\t('at', 'last')\n",
      "6.39\t122\t('have', 'been')\n",
      "\n",
      "Bottom 20 by PMI where pair count>30\n",
      "-0.20\t42\t('in', 'it')\n",
      "-0.20\t370\t('and', 'the')\n",
      "-0.20\t79\t('and', 'that')\n",
      "-0.23\t65\t('not', 'the')\n",
      "-0.30\t53\t('to', 'that')\n",
      "-0.31\t38\t('or', 'the')\n",
      "-0.31\t99\t('and', 'in')\n",
      "-0.31\t31\t('but', 'to')\n",
      "-0.33\t36\t('were', 'the')\n",
      "-0.37\t108\t('and', 'a')\n",
      "-0.43\t31\t('then', 'the')\n",
      "-0.47\t50\t('be', 'the')\n",
      "-0.57\t32\t('and', 'for')\n",
      "-0.65\t33\t('now', 'the')\n",
      "-0.67\t75\t('s', 'the')\n",
      "-0.76\t44\t('and', 'his')\n",
      "-0.79\t47\t('whale', 'the')\n",
      "-0.80\t35\t('the', 'one')\n",
      "-1.06\t32\t('that', 'a')\n",
      "-1.34\t37\t('that', 'of')\n"
     ]
    }
   ],
   "source": [
    "def test_filtered_pmi(PMIsorted, bpd):\n",
    "    '''Test ex3 with values from test1 and test2'''\n",
    "\n",
    "    high_freq = filtered_pmi(PMIsorted, bpd)\n",
    "\n",
    "    print(\"\\nTop 20 by PMI where pair count>30\")\n",
    "    print(\"%s\\t%s\\t%s\"%('PMI','n','pair'))\n",
    "    for t,pmi,n in high_freq[:20]:\n",
    "        print(\"%0.2f\\t%d\\t%s\" % (pmi, n, t))\n",
    "\n",
    "    print(\"\\nBottom 20 by PMI where pair count>30\")\n",
    "    \n",
    "    for t,pmi,n in high_freq[-20:]:\n",
    "        print(\"%0.2f\\t%d\\t%s\" % (pmi, n, t))\n",
    "\n",
    "# TEST EXERCISE 3\n",
    "test_filtered_pmi(PMIsorted, bigramProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a negative score say about a bigram?\n",
    "\n",
    "**Optionally** you can eliminate stop-words from the corpus by applying the second Filter function, then\n",
    "recompute the PMI and investigate the top and last bigrams."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
